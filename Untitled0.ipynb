{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFl3ysuB3w1u",
        "outputId": "070deabc-5f12-446e-b508-e2a01b032019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.9480667114257812\n",
            "Epoch 2, Loss: 1.7545050382614136\n",
            "Epoch 3, Loss: 1.4186619520187378\n",
            "Epoch 4, Loss: 1.0266717672348022\n",
            "Epoch 5, Loss: 0.646578848361969\n",
            "Epoch 6, Loss: 0.37047532200813293\n",
            "Epoch 7, Loss: 0.2599272131919861\n",
            "Epoch 8, Loss: 0.14921578764915466\n",
            "Epoch 9, Loss: 0.0916808471083641\n",
            "Epoch 10, Loss: 0.07249412685632706\n",
            "Epoch 11, Loss: 0.06072143092751503\n",
            "Epoch 12, Loss: 0.023646041750907898\n",
            "Epoch 13, Loss: 0.023737771436572075\n",
            "Epoch 14, Loss: 0.029657524079084396\n",
            "Epoch 15, Loss: 0.01657092012465\n",
            "Epoch 16, Loss: 0.02370321936905384\n",
            "Epoch 17, Loss: 0.004768574610352516\n",
            "Epoch 18, Loss: 0.0015892147785052657\n",
            "Epoch 19, Loss: 0.01793690398335457\n",
            "Epoch 20, Loss: 0.005433916579931974\n",
            "Epoch 21, Loss: 0.018679192289710045\n",
            "Epoch 22, Loss: 0.007864094339311123\n",
            "Epoch 23, Loss: 0.0032112470362335443\n",
            "Epoch 24, Loss: 0.005606761202216148\n",
            "Epoch 25, Loss: 0.021993493661284447\n",
            "Epoch 26, Loss: 0.017509669065475464\n",
            "Epoch 27, Loss: 0.003870381275191903\n",
            "Epoch 28, Loss: 0.01058320514857769\n",
            "Epoch 29, Loss: 0.004758447874337435\n",
            "Epoch 30, Loss: 0.0009148037061095238\n",
            "Epoch 31, Loss: 0.0027802209369838238\n",
            "Epoch 32, Loss: 0.00866124127060175\n",
            "Epoch 33, Loss: 0.018382487818598747\n",
            "Epoch 34, Loss: 0.01584133505821228\n",
            "Epoch 35, Loss: 0.014222229830920696\n",
            "Epoch 36, Loss: 0.004360552411526442\n",
            "Epoch 37, Loss: 0.0019860316533595324\n",
            "Epoch 38, Loss: 0.007020465098321438\n",
            "Epoch 39, Loss: 0.0034635551273822784\n",
            "Epoch 40, Loss: 0.0053939055651426315\n",
            "Epoch 41, Loss: 0.007626255974173546\n",
            "Epoch 42, Loss: 0.0046631633304059505\n",
            "Epoch 43, Loss: 0.004990309942513704\n",
            "Epoch 44, Loss: 0.006130920723080635\n",
            "Epoch 45, Loss: 0.013305431231856346\n",
            "Epoch 46, Loss: 0.0031438760925084352\n",
            "Epoch 47, Loss: 0.005450957454741001\n",
            "Epoch 48, Loss: 0.004308705218136311\n",
            "Epoch 49, Loss: 0.007525079417973757\n",
            "Epoch 50, Loss: 0.005145261064171791\n",
            "Epoch 51, Loss: 0.004220380447804928\n",
            "Epoch 52, Loss: 0.003311299253255129\n",
            "Epoch 53, Loss: 0.005874505266547203\n",
            "Epoch 54, Loss: 0.006632690783590078\n",
            "Epoch 55, Loss: 0.006122201681137085\n",
            "Epoch 56, Loss: 0.0033999276347458363\n",
            "Epoch 57, Loss: 0.005017371848225594\n",
            "Epoch 58, Loss: 0.004101311322301626\n",
            "Epoch 59, Loss: 0.003046487458050251\n",
            "Epoch 60, Loss: 0.003940038848668337\n",
            "Epoch 61, Loss: 0.006559445057064295\n",
            "Epoch 62, Loss: 0.0070059518329799175\n",
            "Epoch 63, Loss: 0.003949694335460663\n",
            "Epoch 64, Loss: 0.004548068158328533\n",
            "Epoch 65, Loss: 0.0052135055884718895\n",
            "Epoch 66, Loss: 0.003841652302071452\n",
            "Epoch 67, Loss: 0.006317623890936375\n",
            "Epoch 68, Loss: 0.0043908352963626385\n",
            "Epoch 69, Loss: 0.00692295515909791\n",
            "Epoch 70, Loss: 0.004705206491053104\n",
            "Epoch 71, Loss: 0.004551441874355078\n",
            "Epoch 72, Loss: 0.003881990909576416\n",
            "Epoch 73, Loss: 0.008408728055655956\n",
            "Epoch 74, Loss: 0.006302230525761843\n",
            "Epoch 75, Loss: 0.005828612484037876\n",
            "Epoch 76, Loss: 0.006880523636937141\n",
            "Epoch 77, Loss: 0.006891145836561918\n",
            "Epoch 78, Loss: 0.0061769792810082436\n",
            "Epoch 79, Loss: 0.004093667026609182\n",
            "Epoch 80, Loss: 0.00590542471036315\n",
            "Epoch 81, Loss: 0.005717533174902201\n",
            "Epoch 82, Loss: 0.008196366019546986\n",
            "Epoch 83, Loss: 0.005057602655142546\n",
            "Epoch 84, Loss: 0.005756970029324293\n",
            "Epoch 85, Loss: 0.007497258484363556\n",
            "Epoch 86, Loss: 0.008340992033481598\n",
            "Epoch 87, Loss: 0.005486797075718641\n",
            "Epoch 88, Loss: 0.006475645117461681\n",
            "Epoch 89, Loss: 0.005658427719026804\n",
            "Epoch 90, Loss: 0.0071898349560797215\n",
            "Epoch 91, Loss: 0.007217083591967821\n",
            "Epoch 92, Loss: 0.006409887690097094\n",
            "Epoch 93, Loss: 0.007890245877206326\n",
            "Epoch 94, Loss: 0.009091610088944435\n",
            "Epoch 95, Loss: 0.010500696487724781\n",
            "Epoch 96, Loss: 0.007332506123930216\n",
            "Epoch 97, Loss: 0.010279526934027672\n",
            "Epoch 98, Loss: 0.01549584697932005\n",
            "Epoch 99, Loss: 0.005893274210393429\n",
            "Epoch 100, Loss: 0.018743934109807014\n",
            "Epoch 101, Loss: 0.013019462116062641\n",
            "Epoch 102, Loss: 0.009017379023134708\n",
            "Epoch 103, Loss: 0.020404120907187462\n",
            "Epoch 104, Loss: 0.007870172150433064\n",
            "Epoch 105, Loss: 0.014578936621546745\n",
            "Epoch 106, Loss: 0.022671975195407867\n",
            "Epoch 107, Loss: 0.01571696624159813\n",
            "Epoch 108, Loss: 0.005849005188792944\n",
            "Epoch 109, Loss: 0.006822449155151844\n",
            "Epoch 110, Loss: 0.009221815504133701\n",
            "Epoch 111, Loss: 0.011147256009280682\n",
            "Epoch 112, Loss: 0.005443842150270939\n",
            "Epoch 113, Loss: 0.006426672451198101\n",
            "Epoch 114, Loss: 0.005653601139783859\n",
            "Epoch 115, Loss: 0.007482352200895548\n",
            "Epoch 116, Loss: 0.009985345415771008\n",
            "Epoch 117, Loss: 0.005371440667659044\n",
            "Epoch 118, Loss: 0.0033185696229338646\n",
            "Epoch 119, Loss: 0.0041489386931061745\n",
            "Epoch 120, Loss: 0.0050687422044575214\n",
            "Epoch 121, Loss: 0.015144728124141693\n",
            "Epoch 122, Loss: 0.003955521620810032\n",
            "Epoch 123, Loss: 0.004271740093827248\n",
            "Epoch 124, Loss: 0.003147874027490616\n",
            "Epoch 125, Loss: 0.0035160325933247805\n",
            "Epoch 126, Loss: 0.004509645979851484\n",
            "Epoch 127, Loss: 0.006433004047721624\n",
            "Epoch 128, Loss: 0.006917076651006937\n",
            "Epoch 129, Loss: 0.003388024168089032\n",
            "Epoch 130, Loss: 0.004757487215101719\n",
            "Epoch 131, Loss: 0.003530611051246524\n",
            "Epoch 132, Loss: 0.007645575329661369\n",
            "Epoch 133, Loss: 0.006337507627904415\n",
            "Epoch 134, Loss: 0.005248179193586111\n",
            "Epoch 135, Loss: 0.0044563268311321735\n",
            "Epoch 136, Loss: 0.004101085010915995\n",
            "Epoch 137, Loss: 0.007981404662132263\n",
            "Epoch 138, Loss: 0.0033191253896802664\n",
            "Epoch 139, Loss: 0.0028865740168839693\n",
            "Epoch 140, Loss: 0.005739969667047262\n",
            "Epoch 141, Loss: 0.010869557969272137\n",
            "Epoch 142, Loss: 0.006112993229180574\n",
            "Epoch 143, Loss: 0.012775282375514507\n",
            "Epoch 144, Loss: 0.006651455070823431\n",
            "Epoch 145, Loss: 0.008967988193035126\n",
            "Epoch 146, Loss: 0.008536598645150661\n",
            "Epoch 147, Loss: 0.006646932568401098\n",
            "Epoch 148, Loss: 0.005148067604750395\n",
            "Epoch 149, Loss: 0.004463700111955404\n",
            "Epoch 150, Loss: 0.005314318463206291\n",
            "Epoch 151, Loss: 0.010612111538648605\n",
            "Epoch 152, Loss: 0.0065039945766329765\n",
            "Epoch 153, Loss: 0.007286708801984787\n",
            "Epoch 154, Loss: 0.0055251773446798325\n",
            "Epoch 155, Loss: 0.007094415836036205\n",
            "Epoch 156, Loss: 0.0049531240947544575\n",
            "Epoch 157, Loss: 0.004487781785428524\n",
            "Epoch 158, Loss: 0.0038410970009863377\n",
            "Epoch 159, Loss: 0.006311533972620964\n",
            "Epoch 160, Loss: 0.005755915772169828\n",
            "Epoch 161, Loss: 0.004917752929031849\n",
            "Epoch 162, Loss: 0.0031366932671517134\n",
            "Epoch 163, Loss: 0.0050585526041686535\n",
            "Epoch 164, Loss: 0.004857000429183245\n",
            "Epoch 165, Loss: 0.005687415599822998\n",
            "Epoch 166, Loss: 0.004506151657551527\n",
            "Epoch 167, Loss: 0.004060495179146528\n",
            "Epoch 168, Loss: 0.0040733166970312595\n",
            "Epoch 169, Loss: 0.006303758360445499\n",
            "Epoch 170, Loss: 0.003070656443014741\n",
            "Epoch 171, Loss: 0.00390238338150084\n",
            "Epoch 172, Loss: 0.0046547818928956985\n",
            "Epoch 173, Loss: 0.004439930431544781\n",
            "Epoch 174, Loss: 0.0035549390595406294\n",
            "Epoch 175, Loss: 0.0035018478520214558\n",
            "Epoch 176, Loss: 0.006571600213646889\n",
            "Epoch 177, Loss: 0.005952328443527222\n",
            "Epoch 178, Loss: 0.005161898210644722\n",
            "Epoch 179, Loss: 0.0055067879147827625\n",
            "Epoch 180, Loss: 0.006747319363057613\n",
            "Epoch 181, Loss: 0.004185408353805542\n",
            "Epoch 182, Loss: 0.005442358553409576\n",
            "Epoch 183, Loss: 0.006221411284059286\n",
            "Epoch 184, Loss: 0.004968743771314621\n",
            "Epoch 185, Loss: 0.006154810078442097\n",
            "Epoch 186, Loss: 0.006201332435011864\n",
            "Epoch 187, Loss: 0.004108408000320196\n",
            "Epoch 188, Loss: 0.007642828393727541\n",
            "Epoch 189, Loss: 0.00428810715675354\n",
            "Epoch 190, Loss: 0.0055336300283670425\n",
            "Epoch 191, Loss: 0.00528533523902297\n",
            "Epoch 192, Loss: 0.006127943750470877\n",
            "Epoch 193, Loss: 0.0049485028721392155\n",
            "Epoch 194, Loss: 0.0035817043390125036\n",
            "Epoch 195, Loss: 0.006437427364289761\n",
            "Epoch 196, Loss: 0.006472737528383732\n",
            "Epoch 197, Loss: 0.004634609911590815\n",
            "Epoch 198, Loss: 0.004969927482306957\n",
            "Epoch 199, Loss: 0.005610645283013582\n",
            "Epoch 200, Loss: 0.0053384131751954556\n",
            "Test Accuracy: 0.808\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ImportError:\n",
        "    # Install torch_geometric if not already installed\n",
        "    !pip install torch_geometric\n",
        "    !pip install torch_scatter torch_sparse torch_cluster torch_spline_conv\n",
        "\n",
        "# Load a sample dataset\n",
        "dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
        "data = dataset[0]\n",
        "\n",
        "# Define the Graph Neural Network (GNN) Model with more layers and Dropout\n",
        "class ImprovedGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedGNN, self).__init__()\n",
        "        # Increased number of neurons in each layer\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, 128)  # First layer: input_dim=128\n",
        "        self.conv2 = GCNConv(128, 64)  # Second layer: 64 neurons\n",
        "        self.conv3 = GCNConv(64, dataset.num_classes)  # Third layer: output layer\n",
        "\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))  # First layer\n",
        "        x = F.relu(self.conv2(x, edge_index))  # Second layer\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.conv3(x, edge_index)  # Third layer\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the model\n",
        "model = ImprovedGNN()\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Added weight decay\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)  # Forward pass\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])  # Compute loss\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update parameters\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Testing function\n",
        "def test():\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    pred = out.argmax(dim=1)\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum().item()\n",
        "    acc = correct / data.test_mask.sum().item()\n",
        "    return acc\n",
        "\n",
        "# Train the model for 200 epochs\n",
        "for epoch in range(200):\n",
        "    loss = train()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        "# Test the model\n",
        "accuracy = test()\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAgvvaYLCbqb",
        "outputId": "df58a126-ff47-48b0-c561-c9fe899ee49e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_geometric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ji3ZHR_Cgx9",
        "outputId": "0e63f75d-98f4-44bb-c92c-62f33bf80fa6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    }
  ]
}